"""
Semantic Scholarì—ì„œ í•™ìˆ  ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³ , ê·¸ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ëª¨ë“ˆ.
httpxë¥¼ ì‚¬ìš©í•˜ì—¬ APIë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ë©°, í•œêµ­ì–´ ì¿¼ë¦¬ë¥¼ ì˜ì–´ë¡œ ìë™ ë²ˆì—­í•˜ê³ ,
ìƒìœ„ 3ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ë³´ê³ .
"""

import os
import re
import tempfile
import asyncio
import time
from typing import Optional, List, Dict, Any, ClassVar

import fitz  # PyMuPDF
import httpx
from langchain.callbacks.manager import (
    AsyncCallbackManagerForToolRun,
    CallbackManagerForToolRun,
)
from langchain.tools import BaseTool
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from ...core.config import settings

# LLM ì¸ìŠ¤í„´ìŠ¤
llm = ChatOpenAI(
    model=settings.OPENAI_MODEL, temperature=0, api_key=settings.OPENAI_API_KEY
)


class SemanticScholarTool(BaseTool):
    """
    Semantic Scholarì—ì„œ ëª¨ë“  ë¶„ì•¼ì˜ í•™ìˆ  ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³ ,
    Open Access PDFê°€ ìˆëŠ” ê²½ìš° ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ëŠ” ë„êµ¬.
    """

    name: str = "semantic_scholar_search"
    description: str = (
        "Use this tool to find and summarize up to 3 academic papers from all fields, including social sciences, medicine, and humanities. "
        "It automatically generates effective English search queries from Korean."
    )

    BASE_URL: ClassVar[str] = "https://api.semanticscholar.org/graph/v1"

    def _search_papers(self, query: str) -> List[Dict[str, Any]]:
        """httpxë¥¼ ì‚¬ìš©í•˜ì—¬ Semantic Scholar APIë¥¼ ì§ì ‘ í˜¸ì¶œ."""
        url = f"{self.BASE_URL}/paper/search"
        params = {
            "query": query,
            "limit": 3,  # 3ê°œë¡œ ì œí•œ
            "fields": "title,abstract,url,year,authors,openAccessPdf",
        }
        headers = (
            {"x-api-key": settings.SEMANTIC_SCHOLAR_API_KEY}
            if settings.SEMANTIC_SCHOLAR_API_KEY
            else {}
        )
        try:
            with httpx.Client(timeout=30.0, headers=headers) as client:
                response = client.get(url, params=params)
                response.raise_for_status()
                data = response.json()
                return data.get("data", [])
        except httpx.TimeoutException:
            print(f"[Semantic Scholar] API í˜¸ì¶œ ì‹¤íŒ¨: íƒ€ì„ì•„ì›ƒ (30ì´ˆ) - {query}")
            return []
        except Exception as e:
            print(f"[Semantic Scholar] API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _translate_query_to_english(self, query: str) -> List[str]:
        """ì‚¬ìš©ìì˜ í•œêµ­ì–´ ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œì˜ ì˜ì–´ ê²€ìƒ‰ì–´ë¡œ ë³€í™˜."""
        print(f"    [ì¿¼ë¦¬ ìƒì„±] í•œêµ­ì–´ ì¿¼ë¦¬ ë¶„ì„ ë° ì˜ì–´ ê²€ìƒ‰ì–´ ìƒì„± ì‹œë„: '{query}'")
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "You are an expert academic researcher. Your task is to analyze the following Korean user query and generate up to 3 "
                    "diverse and effective English search keyword phrases for an academic database. Provide only the keyword phrases, "
                    "each on a new line, without any numbering or extra text.",
                ),
                ("human", "{korean_query}"),
            ]
        )
        chain = prompt | llm
        response = chain.invoke({"korean_query": query})
        queries = [q.strip() for q in response.content.split("\n") if q.strip()]
        print(f"    [ì¿¼ë¦¬ ìƒì„±] ì™„ë£Œ: {queries}")
        return queries

    def _analyze_paper(self, paper: dict) -> Optional[str]:
        """ë‹¨ì¼ ë…¼ë¬¸ì„ ë¶„ì„í•˜ê³  ìš”ì•½."""
        pdf_url = (
            paper.get("openAccessPdf", {}).get("url")
            if paper.get("openAccessPdf")
            else None
        )

        if pdf_url:
            print(f"    [PDF ë°œê²¬] Open Access PDF ë°œê²¬: {paper.get('title', 'N/A')}")
            with tempfile.TemporaryDirectory() as temp_dir:
                pdf_path = self._download_pdf(pdf_url, temp_dir)
                if pdf_path:
                    extracted_text = self._extract_intro_and_conclusion(pdf_path)
                    if (
                        "Failed" not in extracted_text
                        and "Could not find" not in extracted_text
                    ):
                        return self._summarize_text(
                            extracted_text, paper.get("title", "N/A")
                        )

        print(
            f"    [ì´ˆë¡ ìš”ì•½] PDFë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ì–´ ì´ˆë¡ì„ ìš”ì•½í•©ë‹ˆë‹¤: {paper.get('title', 'N/A')}"
        )
        return self._summarize_text(
            paper.get("abstract", ""), paper.get("title", "N/A")
        )

    # pylint: disable=arguments-differ
    def _run(
        self,
        query: str,
        run_manager: Optional[
            CallbackManagerForToolRun
        ] = None,  # pylint: disable=unused-argument
    ) -> str:
        """ì§€ëŠ¥í˜• ì¿¼ë¦¬ ìƒì„± -> ìˆœì°¨ ê²€ìƒ‰ -> ìƒìœ„ 3ê°œ ê²°ê³¼ ì¢…í•© ë¡œì§ ì‹¤í–‰."""
        process_log = [f"ğŸ” **'{query}'ì— ëŒ€í•œ ì „ì²´ í•™ìˆ  ìë£Œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...**\n"]

        english_queries = self._translate_query_to_english(query)
        process_log.append(
            "ğŸ”„ **ê²€ìƒ‰ì–´ ìƒì„±:** " + ", ".join(f"'{q}'" for q in english_queries)
        )

        all_results = []
        time.sleep(1)  # ì²« API í˜¸ì¶œ ì „ ì„ ì œì  ë”œë ˆì´
        for eq in english_queries:
            print(f"[Semantic Scholar] API í˜¸ì¶œ ì‹œì‘: '{eq}'")
            papers = self._search_papers(eq)
            print(f"[Semantic Scholar] API í˜¸ì¶œ ì™„ë£Œ: {len(papers)}ê°œ ê²°ê³¼ ìˆ˜ì‹ .")
            if papers:
                all_results.extend(papers)
            if len(all_results) >= 3:
                break
            time.sleep(2)

        if not all_results:
            return "í•´ë‹¹ ì£¼ì œì— ëŒ€í•œ ë…¼ë¬¸ì„ Semantic Scholarì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        process_log.append(
            f"\nğŸ“„ **ì´ {len(all_results[:3])}ê°œì˜ ê´€ë ¨ ë…¼ë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤.**\n"
        )

        for i, paper in enumerate(all_results[:3]):  # ìµœëŒ€ 3ê°œë§Œ ë¶„ì„
            process_log.append(f"--- \n### **ë¶„ì„ {i+1}: {paper.get('title', 'N/A')}**")

            korean_summary = self._analyze_paper(paper)
            process_log.append(korean_summary or "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            process_log.append(
                f"\n- **ì €ì**: {', '.join(author['name'] for author in paper.get('authors', []))}"
            )
            process_log.append(f"- **ê²Œì¬ ì—°ë„**: {paper.get('year', 'N/A')}")
            process_log.append(f"- **ë§í¬**: {paper.get('url', 'N/A')}\n")

        return "\n".join(process_log)

    def _download_pdf(self, pdf_url: str, dir_path: str) -> Optional[str]:
        """PDFë¥¼ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ë‹¤ìš´ë¡œë“œí•˜ê³  íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜."""
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        try:
            with httpx.Client(
                timeout=60.0, headers=headers, follow_redirects=True
            ) as client:
                response = client.get(pdf_url)
                response.raise_for_status()
                filepath = os.path.join(dir_path, "temp_paper.pdf")
                with open(filepath, "wb") as f:
                    f.write(response.content)
                return filepath
        except Exception:  # pylint: disable=broad-exception-caught
            return None

    def _extract_intro_and_conclusion(self, filepath: str) -> str:
        """PDFì—ì„œ ì„œë¡ ê³¼ ê²°ë¡  ë¶€ë¶„ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ."""
        try:
            doc = fitz.open(filepath)
            full_text = " ".join(
                page.get_text("text", sort=True) for page in doc
            ).replace("\n", " ")
            doc.close()
            intro_start_match = re.search(
                r"(1\s*\.?\s*)?INTRODUCTION", full_text, re.IGNORECASE
            )
            if not intro_start_match:
                return "Could not find introduction."
            intro_text = full_text[intro_start_match.start() :]
            intro_end_match = re.search(
                r"2\s*\.?\s*(BACKGROUND|RELATED WORK|PRELIMINARIES)",
                intro_text,
                re.IGNORECASE,
            )
            introduction = (
                intro_text[: intro_end_match.start()]
                if intro_end_match
                else intro_text[:4000]
            )
            conclusion_text = ""
            conclusion_start_match = re.search(
                r"CONCLUSION|DISCUSSION", full_text, re.IGNORECASE
            )
            if conclusion_start_match:
                references_start_match = re.search(
                    r"REFERENCES|ACKNOWLEDGEMENTS",
                    full_text[conclusion_start_match.start() :],
                    re.IGNORECASE,
                )
                if references_start_match:
                    conclusion_text = full_text[
                        conclusion_start_match.start() : conclusion_start_match.start()
                        + references_start_match.start()
                    ]
                else:
                    conclusion_text = full_text[conclusion_start_match.start() :]
            return f"--- INTRODUCTION ---\n{introduction}\n\n--- CONCLUSION ---\n{conclusion_text}"
        except Exception:  # pylint: disable=broad-exception-caught
            return "Failed to extract text from PDF."

    def _summarize_text(self, text: str, title: str) -> str:
        """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½."""
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "You are an expert research paper analyst. Your task is to synthesize the provided Introduction and Conclusion "
                    "of an academic paper into a comprehensive yet easy-to-understand summary in Korean. "
                    "Focus on the paper's core problem, methodology, key findings, and implications.",
                ),
                (
                    "human",
                    "Please summarize the following content from the paper titled '{title}':\n\n{text}",
                ),
            ]
        )
        chain = prompt | llm
        summary = chain.invoke({"text": text, "title": title})
        return summary.content

    # pylint: disable=arguments-differ
    async def _arun(
        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None
    ) -> str:
        """ë¹„ë™ê¸° í™˜ê²½ì—ì„œ ë™ê¸° í•¨ìˆ˜ì¸ _runì„ ì‹¤í–‰."""
        return await asyncio.to_thread(self._run, query, run_manager=run_manager)
