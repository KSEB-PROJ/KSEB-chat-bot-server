"""arXiv.orgì—ì„œ í•™ìˆ  ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³ , PDF ë³¸ë¬¸ì„ ë¶„ì„í•˜ì—¬
ì‹¬ì¸µì ì¸ í•œêµ­ì–´ ìš”ì•½ì„ ì œê³µí•˜ëŠ” ëª¨ë“ˆ.
ì‚¬ìš©ìì˜ í•œêµ­ì–´ ì¿¼ë¦¬ë¥¼ ì˜ì–´ ê²€ìƒ‰ì–´ë“¤ë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ,
ìƒìœ„ 3ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ë³´ê³ .
"""
import os
import re
import tempfile
import asyncio
import time
from typing import Optional, List

import arxiv
import fitz  # PyMuPDF
import httpx
from langchain.callbacks.manager import (
    AsyncCallbackManagerForToolRun,
    CallbackManagerForToolRun,
)
from langchain.tools import BaseTool
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from ...core.config import settings

# LLM ì¸ìŠ¤í„´ìŠ¤
llm = ChatOpenAI(
    model=settings.OPENAI_MODEL,
    temperature=0,
    api_key=settings.OPENAI_API_KEY
)

class AdvancedArxivTool(BaseTool):
    """
    arXiv.orgì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³ , PDF ë³¸ë¬¸ì„ ë¶„ì„í•˜ì—¬
    ì‹¬ì¸µì ì¸ í•œêµ­ì–´ ìš”ì•½ì„ ì œê³µí•˜ëŠ” ë„êµ¬.
    """
    name: str = "advanced_arxiv_search"
    description: str = (
        "Use this tool to find and deeply summarize up to 3 academic papers from arXiv, especially for STEM fields like Computer Science and AI. "
        "It automatically translates Korean queries to English."
    )

    def _translate_query_to_english(self, query: str) -> List[str]:
        """ì‚¬ìš©ìì˜ í•œêµ­ì–´ ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œì˜ íš¨ê³¼ì ì¸ ì˜ì–´ ê²€ìƒ‰ì–´ë¡œ ë³€í™˜."""
        print(f"    [ì¿¼ë¦¬ ìƒì„±] í•œêµ­ì–´ ì¿¼ë¦¬ ë¶„ì„ ë° ì˜ì–´ ê²€ìƒ‰ì–´ ìƒì„± ì‹œë„: '{query}'")
        prompt = ChatPromptTemplate.from_messages([
            (
                "system",
                "You are an expert academic researcher. Your task is to analyze the following Korean user query and generate up to 3 "
                "diverse and effective English search keyword phrases for an academic database. Provide only the keyword phrases, "
                "each on a new line, without any numbering or extra text."
            ),
            ("human", "{korean_query}"),
        ])
        chain = prompt | llm
        response = chain.invoke({"korean_query": query})
        queries = [q.strip() for q in response.content.split('\n') if q.strip()]
        print(f"    [ì¿¼ë¦¬ ìƒì„±] ì™„ë£Œ: {queries}")
        return queries

    def _analyze_paper(self, paper: arxiv.Result) -> Optional[str]:
        """ë‹¨ì¼ ë…¼ë¬¸ì„ ë‹¤ìš´ë¡œë“œ, ë¶„ì„, ìš”ì•½í•˜ëŠ” íŒŒì´í”„ë¼ì¸."""
        process_log = []
        with tempfile.TemporaryDirectory() as temp_dir:
            process_log.append("    - ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ ì‹œë„...")
            pdf_path = self._download_pdf(paper.pdf_url, temp_dir)
            if not pdf_path:
                process_log.append("    - âš ï¸ **ë¶„ì„ ì‹¤íŒ¨:** PDFë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return "\n".join(process_log)

            process_log.append("    - ğŸ“‘ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë¶„ì„ ì‹œë„...")
            extracted_text = self._extract_intro_and_conclusion(pdf_path)
            if "Failed" in extracted_text or "Could not find" in extracted_text:
                process_log.append("    - âš ï¸ **ë¶„ì„ ì‹¤íŒ¨:** PDF êµ¬ì¡°ê°€ ë³µì¡í•˜ì—¬ ì„œë¡ /ê²°ë¡ ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ˆë¡ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                extracted_text = paper.summary
            else:
                process_log.append("    - âœ… PDF ë¶„ì„ ì™„ë£Œ.")

            process_log.append("    - âœï¸ í•œêµ­ì–´ ìš”ì•½ ìƒì„± ì‹œë„...")
            summary = self._summarize_text(extracted_text, paper.title)
            process_log.append("    - âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ.")
            process_log.append(f"\n{summary}") # ìµœì¢… ìš”ì•½ë³¸ ì¶”ê°€

            return "\n".join(process_log)

    # pylint: disable=arguments-differ
    def _run(
        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None  # pylint: disable=unused-argument
    ) -> str:
        """'ì§€ëŠ¥í˜• ì¿¼ë¦¬ ìƒì„± -> ìˆœì°¨ ê²€ìƒ‰ -> ìƒìœ„ 3ê°œ ê²°ê³¼ ì¢…í•© ë¡œì§ ì‹¤í–‰."""
        process_log = [f"ğŸ” **'{query}'ì— ëŒ€í•œ arXiv ë…¼ë¬¸ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...**\n"]
        
        english_queries = self._translate_query_to_english(query)
        process_log.append("ğŸ”„ **ê²€ìƒ‰ì–´ ìƒì„±:** " + ", ".join(f"'{q}'" for q in english_queries))

        all_results = []
        for eq in english_queries:
            print(f"[Arxiv Search] API í˜¸ì¶œ ì‹œì‘: '{eq}'")
            try:
                search = arxiv.Search(query=eq, max_results=3, sort_by=arxiv.SortCriterion.Relevance)
                results = list(search.results())
                print(f"[Arxiv Search] API í˜¸ì¶œ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ìˆ˜ì‹ .")
                if results:
                    all_results.extend(results)
                if len(all_results) >= 3:
                    break
                time.sleep(2)
            except Exception as e:  # pylint: disable=broad-exception-caught
                print(f"[Arxiv Search] API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                process_log.append(f"\n- âš ï¸ **ì˜¤ë¥˜:** '{eq}' ê²€ìƒ‰ ì¤‘ arXiv.org ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                continue
        
        if not all_results:
            return "í•´ë‹¹ ì£¼ì œì— ëŒ€í•œ ë…¼ë¬¸ì„ arXivì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        process_log.append(f"\nğŸ“„ **ì´ {len(all_results[:3])}ê°œì˜ ê´€ë ¨ ë…¼ë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤.**\n")

        for i, paper in enumerate(all_results[:3]):
            process_log.append(f"--- \n### **ë¶„ì„ {i+1}: {paper.title}**")
            
            analysis_report = self._analyze_paper(paper)
            process_log.append(analysis_report or "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            process_log.append(f"\n- **ì €ì**: {', '.join(author.name for author in paper.authors)}")
            process_log.append(f"- **ê²Œì¬ì¼**: {paper.published.date()}")
            process_log.append(f"- **ë§í¬**: {paper.entry_id}\n")

        return "\n".join(process_log)

    def _download_pdf(self, pdf_url: str, dir_path: str) -> Optional[str]:
        """PDFë¥¼ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ë‹¤ìš´ë¡œë“œí•˜ê³  íŒŒì¼ ê²½ë¡œ ë°˜í™˜."""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        try:
            with httpx.Client(timeout=60.0, headers=headers, follow_redirects=True) as client:
                response = client.get(pdf_url)
                response.raise_for_status()
                filepath = os.path.join(dir_path, "temp_paper.pdf")
                with open(filepath, "wb") as f:
                    f.write(response.content)
                return filepath
        except Exception:  # pylint: disable=broad-exception-caught
            return None

    def _extract_intro_and_conclusion(self, filepath: str) -> str:
        """PDFì—ì„œ ì„œë¡ ê³¼ ê²°ë¡  ë¶€ë¶„ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ."""
        try:
            doc = fitz.open(filepath)
            full_text = " ".join(page.get_text("text", sort=True) for page in doc).replace('\n', ' ')
            doc.close()
            intro_start_match = re.search(r"(1\s*\.?\s*)?INTRODUCTION", full_text, re.IGNORECASE)
            if not intro_start_match:
                return "Could not find introduction."
            intro_text = full_text[intro_start_match.start():]
            intro_end_match = re.search(r"2\s*\.?\s*(BACKGROUND|RELATED WORK|PRELIMINARIES)", intro_text, re.IGNORECASE)
            introduction = intro_text[:intro_end_match.start()] if intro_end_match else intro_text[:4000]
            conclusion_text = ""
            conclusion_start_match = re.search(r"CONCLUSION|DISCUSSION", full_text, re.IGNORECASE)
            if conclusion_start_match:
                references_start_match = re.search(r"REFERENCES|ACKNOWLEDGEMENTS", full_text[conclusion_start_match.start():], re.IGNORECASE)
                if references_start_match:
                    conclusion_end_index = conclusion_start_match.start() + references_start_match.start()
                    conclusion_text = full_text[conclusion_start_match.start():conclusion_end_index]
                else:
                    conclusion_text = full_text[conclusion_start_match.start():]
            return f"--- INTRODUCTION ---\n{introduction}\n\n--- CONCLUSION ---\n{conclusion_text}"
        except Exception:  # pylint: disable=broad-exception-caught
            return "Failed to extract text from PDF."

    def _summarize_text(self, text: str, title: str) -> str:
        """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½."""
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a professional academic reviewer. Your task is to analyze the provided text (Introduction and Conclusion of a paper) and write a detailed analysis report in Korean, structured with the following sections:\n\n"
             "**1. ë¬¸ì œ ì œê¸° (Problem Statement):** ì´ ì—°êµ¬ê°€ í•´ê²°í•˜ê³ ì í•˜ëŠ” í•µì‹¬ ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€?\n"
             "**2. ì œì•ˆ ë°©ë²•ë¡  (Proposed Method):** ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì–´ë–¤ ë…ì°½ì ì¸ ë°©ë²•ì´ë‚˜ ì ‘ê·¼ë²•ì„ ì œì•ˆí•˜ëŠ”ê°€?\n"
             "**3. í•µì‹¬ ê²°ê³¼ ë° ì˜ì˜ (Key Results & Significance)::** ì—°êµ¬ë¥¼ í†µí•´ ë¬´ì—‡ì„ ë°œê²¬í–ˆìœ¼ë©°, ì´ ê²°ê³¼ê°€ í•´ë‹¹ ë¶„ì•¼ì— ì–´ë–¤ ì¤‘ìš”í•œ ê¸°ì—¬ë¥¼ í•˜ëŠ”ê°€? (ì˜ˆ: ì„±ëŠ¥ í–¥ìƒ, ìƒˆë¡œìš´ ê°€ëŠ¥ì„± ì œì‹œ ë“±)\n"
             "**4. ì˜ˆìƒ í™œìš© ë¶„ì•¼ (Potential Applications):** ì´ ì—°êµ¬ ê²°ê³¼ê°€ ì‹¤ì œë¡œ ì–´ë””ì— ì‘ìš©ë  ìˆ˜ ìˆëŠ”ê°€?"),
            ("human", "Please analyze the following content from the paper titled '{title}':\n\n{text}"),
        ])
        chain = prompt | llm
        summary = chain.invoke({"text": text, "title": title})
        return summary.content

    # pylint: disable=arguments-differ
    async def _arun(
        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None
    ) -> str:
        """ë¹„ë™ê¸° í™˜ê²½ì—ì„œ ë™ê¸° í•¨ìˆ˜ì¸ _runì„ ì‹¤í–‰."""
        return await asyncio.to_thread(self._run, query, run_manager=run_manager)
